{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac_client import Client\n",
    "from odc.stac import load\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import folium\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import odc.geo.xr  # noqa: F401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC Catalog URL\n",
    "catalog = \"https://earth-search.aws.element84.com/v1\"\n",
    "\n",
    "# Create a STAC Client\n",
    "client = Client.open(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location is north of Kuching, in lat/lon order\n",
    "ll = (1.57247,110.18403)\n",
    "ur = (1.82405,110.56703)\n",
    "bbox = (ll[1], ll[0], ur[1], ur[0])\n",
    "\n",
    "# Three months of data\n",
    "datetime = \"2024-07/2024-09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data\n",
    "data_url = \"https://raw.githubusercontent.com/nick-murray/coastTrain/main/data/coastTrain_v1_0.geojson\"\n",
    "\n",
    "# gdf = gpd.read_file(data_url, bbox=bbox)\n",
    "\n",
    "gdf.explore(column=\"Ecosys_Typ\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for Sentinel-2 data\n",
    "items = client.search(\n",
    "    collections=[\"sentinel-2-c1-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=datetime,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 50}},\n",
    ").item_collection()\n",
    "\n",
    "print(f\"Found {len(items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into an xarray Dataset\n",
    "data = load(\n",
    "    items,\n",
    "    measurements=[\"red\", \"green\", \"blue\", \"nir08\", \"swir16\", \"scl\"],\n",
    "    bbox=bbox,\n",
    "    chunks={\"x\": 2048, \"y\": 2048},\n",
    "    groupby=\"solar_day\",\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out clouds and scale values\n",
    "\n",
    "# Apply Sentinel-2 cloud mask\n",
    "# 1: defective, 3: shadow, 9: high confidence cloud, 10: thin cirrus\n",
    "mask_flags = [1, 3, 9, 10]\n",
    "\n",
    "cloud_mask = ~data.scl.isin(mask_flags)\n",
    "masked = data.where(cloud_mask)\n",
    "\n",
    "# Apply scaling\n",
    "scaled = (masked.where(masked != 0) * 0.0001).clip(0, 1)\n",
    "\n",
    "# Add some indices\n",
    "scaled[\"ndvi\"] = (scaled.nir08 - scaled.red) / (scaled.nir08 + scaled.red)\n",
    "scaled[\"ndwi\"] = (scaled.green - scaled.nir08) / (scaled.green + scaled.nir08)\n",
    "\n",
    "scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise one date, to make sure it looks good. This example doesn't look very good\n",
    "# which highlights the difficulty of workingw with Sentinel-2!\n",
    "\n",
    "scaled.isel(time=0).odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a median composite, which should get rid of most of the remaining clouds\n",
    "\n",
    "median = scaled.median(\"time\").compute()\n",
    "\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median.odc.explore(vmin=0, vmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get observed values onto the training data\n",
    "training = gdf.to_crs(median.odc.geobox.crs)\n",
    "training_da = training.assign(x=training.geometry.x, y=training.geometry.y).to_xarray()\n",
    "\n",
    "training_values = (\n",
    "    median.sel(training_da[[\"x\", \"y\"]], method=\"nearest\").squeeze().compute().to_pandas()\n",
    ")\n",
    "\n",
    "training_array = pd.concat([training[\"Class\"], training_values], axis=1)\n",
    "training_array = training_array.drop(\n",
    "    columns=[\n",
    "        \"y\",\n",
    "        \"x\",\n",
    "        \"spatial_ref\",\n",
    "    ]\n",
    ")\n",
    "# Drop rows where there are any NaNs\n",
    "training_array = training_array.dropna()\n",
    "\n",
    "training_array.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier and fit it to the training data\n",
    "\n",
    "# The training data is everything after the first column\n",
    "training_data = np.array(training_array)[:, 1:]\n",
    "\n",
    "# The classes are the first column\n",
    "values = np.array(training_array)[:, 0]\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "model = classifier.fit(training_data, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run a prediction using the model\n",
    "\n",
    "stacked_arrays = median.to_array().stack(dims=[\"y\", \"x\"]).transpose()\n",
    "predicted = model.predict(stacked_arrays)\n",
    "array = predicted.reshape(len(median.y), len(median.x))\n",
    "\n",
    "predicted_da = xr.DataArray(\n",
    "    array, coords={\"x\": masked.x, \"y\": masked.y}, dims=[\"y\", \"x\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all on a single interactive map\n",
    "\n",
    "center = [np.mean([ll[0], ur[0]]), np.mean([ll[1], ur[1]])]\n",
    "m = folium.Map(location=center, zoom_start=11)\n",
    "\n",
    "median.odc.to_rgba(vmin=0, vmax=0.3).odc.add_to(m, name=\"Median Composite\")\n",
    "predicted_da.odc.add_to(m, name=\"Predicted\")\n",
    "gdf.explore(m=m, column=\"Ecosys_Typ\", legend=True, name=\"Training Data\")\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the results, do they make sense?\n",
    "\n",
    "Next steps are to fine tune the data. Perhaps download the points for this\n",
    "region of interest as well as the RGB image and add and remove points until\n",
    "there is a more representative training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
